{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Santander Customer Transaction Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "a-QFQcSYsjrF",
    "outputId": "02247c45-4ca5-4f5f-e3f7-68326dc71aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (200000, 202)\n",
      "Shape of testing data: (200000, 201)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./sample_data/train.csv')\n",
    "test = pd.read_csv('./sample_data/test.csv')\n",
    "\n",
    "print('Shape of training data: ' + str(train.shape))\n",
    "print('Shape of testing data: ' + str(test.shape))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing value in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "1ED8shelt-cV",
    "outputId": "79594d76-edcd-41e7-f88d-b5969fbc1428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any null value in train dataset: False\n",
      "Is there any null value in test dataset: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>...</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>7.567236</td>\n",
       "      <td>0.394340</td>\n",
       "      <td>-3.245596</td>\n",
       "      <td>14.023978</td>\n",
       "      <td>8.530232</td>\n",
       "      <td>7.537606</td>\n",
       "      <td>14.573126</td>\n",
       "      <td>9.333264</td>\n",
       "      <td>-5.696731</td>\n",
       "      <td>15.244013</td>\n",
       "      <td>12.438567</td>\n",
       "      <td>13.290894</td>\n",
       "      <td>17.257883</td>\n",
       "      <td>4.305430</td>\n",
       "      <td>3.019540</td>\n",
       "      <td>10.584400</td>\n",
       "      <td>13.667496</td>\n",
       "      <td>-4.055133</td>\n",
       "      <td>-1.137908</td>\n",
       "      <td>5.532980</td>\n",
       "      <td>5.053874</td>\n",
       "      <td>-7.687740</td>\n",
       "      <td>10.393046</td>\n",
       "      <td>-0.512886</td>\n",
       "      <td>14.774147</td>\n",
       "      <td>11.434250</td>\n",
       "      <td>3.842499</td>\n",
       "      <td>2.187230</td>\n",
       "      <td>5.868899</td>\n",
       "      <td>10.642131</td>\n",
       "      <td>...</td>\n",
       "      <td>24.259300</td>\n",
       "      <td>5.633293</td>\n",
       "      <td>5.362896</td>\n",
       "      <td>11.002170</td>\n",
       "      <td>-2.871906</td>\n",
       "      <td>19.315753</td>\n",
       "      <td>2.963335</td>\n",
       "      <td>-4.151155</td>\n",
       "      <td>4.937124</td>\n",
       "      <td>5.636008</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>-0.831777</td>\n",
       "      <td>19.817094</td>\n",
       "      <td>-0.677967</td>\n",
       "      <td>20.210677</td>\n",
       "      <td>11.640613</td>\n",
       "      <td>-2.799585</td>\n",
       "      <td>11.882933</td>\n",
       "      <td>-1.014064</td>\n",
       "      <td>2.591444</td>\n",
       "      <td>-2.741666</td>\n",
       "      <td>10.085518</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>8.769088</td>\n",
       "      <td>12.756676</td>\n",
       "      <td>-3.983261</td>\n",
       "      <td>8.970274</td>\n",
       "      <td>-10.335043</td>\n",
       "      <td>15.377174</td>\n",
       "      <td>0.746072</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>1.235070</td>\n",
       "      <td>5.500793</td>\n",
       "      <td>5.970253</td>\n",
       "      <td>0.190059</td>\n",
       "      <td>4.639536</td>\n",
       "      <td>2.247908</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>2.557421</td>\n",
       "      <td>6.712612</td>\n",
       "      <td>7.851370</td>\n",
       "      <td>7.996694</td>\n",
       "      <td>5.876254</td>\n",
       "      <td>8.196564</td>\n",
       "      <td>2.847958</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>3.777245</td>\n",
       "      <td>0.285535</td>\n",
       "      <td>5.922210</td>\n",
       "      <td>1.523714</td>\n",
       "      <td>0.783367</td>\n",
       "      <td>2.615942</td>\n",
       "      <td>7.965198</td>\n",
       "      <td>2.159891</td>\n",
       "      <td>2.587830</td>\n",
       "      <td>4.322325</td>\n",
       "      <td>0.541614</td>\n",
       "      <td>5.179559</td>\n",
       "      <td>3.119978</td>\n",
       "      <td>2.249730</td>\n",
       "      <td>4.278903</td>\n",
       "      <td>...</td>\n",
       "      <td>10.880263</td>\n",
       "      <td>0.217938</td>\n",
       "      <td>1.419612</td>\n",
       "      <td>5.262056</td>\n",
       "      <td>5.457784</td>\n",
       "      <td>5.024182</td>\n",
       "      <td>0.369684</td>\n",
       "      <td>7.798020</td>\n",
       "      <td>3.105986</td>\n",
       "      <td>0.369437</td>\n",
       "      <td>4.424621</td>\n",
       "      <td>5.378008</td>\n",
       "      <td>8.674171</td>\n",
       "      <td>5.966674</td>\n",
       "      <td>7.136427</td>\n",
       "      <td>2.892167</td>\n",
       "      <td>7.513939</td>\n",
       "      <td>2.628895</td>\n",
       "      <td>8.579810</td>\n",
       "      <td>2.798956</td>\n",
       "      <td>5.261243</td>\n",
       "      <td>1.371862</td>\n",
       "      <td>8.963434</td>\n",
       "      <td>4.474924</td>\n",
       "      <td>9.318280</td>\n",
       "      <td>4.725167</td>\n",
       "      <td>3.189759</td>\n",
       "      <td>11.574708</td>\n",
       "      <td>3.944604</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>-20.731300</td>\n",
       "      <td>-26.095000</td>\n",
       "      <td>13.434600</td>\n",
       "      <td>-6.011100</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>13.076900</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>-33.380200</td>\n",
       "      <td>-10.664200</td>\n",
       "      <td>-12.402500</td>\n",
       "      <td>-5.432200</td>\n",
       "      <td>-10.089000</td>\n",
       "      <td>-5.322500</td>\n",
       "      <td>1.209800</td>\n",
       "      <td>-0.678400</td>\n",
       "      <td>12.720000</td>\n",
       "      <td>-24.243100</td>\n",
       "      <td>-6.166800</td>\n",
       "      <td>2.089600</td>\n",
       "      <td>-4.787200</td>\n",
       "      <td>-34.798400</td>\n",
       "      <td>2.140600</td>\n",
       "      <td>-8.986100</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>9.816900</td>\n",
       "      <td>-16.513600</td>\n",
       "      <td>-8.095100</td>\n",
       "      <td>-1.183400</td>\n",
       "      <td>-6.337100</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.452200</td>\n",
       "      <td>4.852600</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>-6.531700</td>\n",
       "      <td>-19.997700</td>\n",
       "      <td>3.816700</td>\n",
       "      <td>1.851200</td>\n",
       "      <td>-35.969500</td>\n",
       "      <td>-5.250200</td>\n",
       "      <td>4.258800</td>\n",
       "      <td>-14.506000</td>\n",
       "      <td>-22.479300</td>\n",
       "      <td>-11.453300</td>\n",
       "      <td>-22.748700</td>\n",
       "      <td>-2.995300</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>-29.116500</td>\n",
       "      <td>4.952100</td>\n",
       "      <td>-29.273400</td>\n",
       "      <td>-7.856100</td>\n",
       "      <td>-22.037400</td>\n",
       "      <td>5.416500</td>\n",
       "      <td>-26.001100</td>\n",
       "      <td>-4.808200</td>\n",
       "      <td>-18.489700</td>\n",
       "      <td>-22.583300</td>\n",
       "      <td>-3.022300</td>\n",
       "      <td>-47.753600</td>\n",
       "      <td>4.412300</td>\n",
       "      <td>-2.554300</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>6.618800</td>\n",
       "      <td>-3.594950</td>\n",
       "      <td>-7.510600</td>\n",
       "      <td>13.894000</td>\n",
       "      <td>5.072800</td>\n",
       "      <td>5.781875</td>\n",
       "      <td>14.262800</td>\n",
       "      <td>7.452275</td>\n",
       "      <td>-10.476225</td>\n",
       "      <td>9.177950</td>\n",
       "      <td>6.276475</td>\n",
       "      <td>8.627800</td>\n",
       "      <td>11.551000</td>\n",
       "      <td>2.182400</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>7.613000</td>\n",
       "      <td>13.456400</td>\n",
       "      <td>-8.321725</td>\n",
       "      <td>-2.307900</td>\n",
       "      <td>4.992100</td>\n",
       "      <td>3.171700</td>\n",
       "      <td>-13.766175</td>\n",
       "      <td>8.870000</td>\n",
       "      <td>-2.500875</td>\n",
       "      <td>11.456300</td>\n",
       "      <td>11.032300</td>\n",
       "      <td>0.116975</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>4.125475</td>\n",
       "      <td>7.591050</td>\n",
       "      <td>...</td>\n",
       "      <td>15.696125</td>\n",
       "      <td>5.470500</td>\n",
       "      <td>4.326100</td>\n",
       "      <td>7.029600</td>\n",
       "      <td>-7.094025</td>\n",
       "      <td>15.744550</td>\n",
       "      <td>2.699000</td>\n",
       "      <td>-9.643100</td>\n",
       "      <td>2.703200</td>\n",
       "      <td>5.374600</td>\n",
       "      <td>-3.258500</td>\n",
       "      <td>-4.720350</td>\n",
       "      <td>13.731775</td>\n",
       "      <td>-5.009525</td>\n",
       "      <td>15.064600</td>\n",
       "      <td>9.371600</td>\n",
       "      <td>-8.386500</td>\n",
       "      <td>9.808675</td>\n",
       "      <td>-7.395700</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>-6.673900</td>\n",
       "      <td>9.084700</td>\n",
       "      <td>-6.064425</td>\n",
       "      <td>5.423100</td>\n",
       "      <td>5.663300</td>\n",
       "      <td>-7.360000</td>\n",
       "      <td>6.715200</td>\n",
       "      <td>-19.205125</td>\n",
       "      <td>12.501550</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>7.629600</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>-3.286950</td>\n",
       "      <td>14.025500</td>\n",
       "      <td>8.604250</td>\n",
       "      <td>7.520300</td>\n",
       "      <td>14.574100</td>\n",
       "      <td>9.232050</td>\n",
       "      <td>-5.666350</td>\n",
       "      <td>15.196250</td>\n",
       "      <td>12.453900</td>\n",
       "      <td>13.196800</td>\n",
       "      <td>17.234250</td>\n",
       "      <td>4.275150</td>\n",
       "      <td>3.008650</td>\n",
       "      <td>10.380350</td>\n",
       "      <td>13.662500</td>\n",
       "      <td>-4.196900</td>\n",
       "      <td>-1.132100</td>\n",
       "      <td>5.534850</td>\n",
       "      <td>4.950200</td>\n",
       "      <td>-7.411750</td>\n",
       "      <td>10.365650</td>\n",
       "      <td>-0.497650</td>\n",
       "      <td>14.576000</td>\n",
       "      <td>11.435200</td>\n",
       "      <td>3.917750</td>\n",
       "      <td>2.198000</td>\n",
       "      <td>5.900650</td>\n",
       "      <td>10.562700</td>\n",
       "      <td>...</td>\n",
       "      <td>23.864500</td>\n",
       "      <td>5.633500</td>\n",
       "      <td>5.359700</td>\n",
       "      <td>10.788700</td>\n",
       "      <td>-2.637800</td>\n",
       "      <td>19.270800</td>\n",
       "      <td>2.960200</td>\n",
       "      <td>-4.011600</td>\n",
       "      <td>4.761600</td>\n",
       "      <td>5.634300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.807350</td>\n",
       "      <td>19.748000</td>\n",
       "      <td>-0.569750</td>\n",
       "      <td>20.206100</td>\n",
       "      <td>11.679800</td>\n",
       "      <td>-2.538450</td>\n",
       "      <td>11.737250</td>\n",
       "      <td>-0.942050</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>-2.688800</td>\n",
       "      <td>10.036050</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>12.521000</td>\n",
       "      <td>-3.946950</td>\n",
       "      <td>8.902150</td>\n",
       "      <td>-10.209750</td>\n",
       "      <td>15.239450</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>8.584425</td>\n",
       "      <td>4.382925</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>14.164200</td>\n",
       "      <td>12.274775</td>\n",
       "      <td>9.270425</td>\n",
       "      <td>14.874500</td>\n",
       "      <td>11.055900</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>21.013325</td>\n",
       "      <td>18.433300</td>\n",
       "      <td>17.879400</td>\n",
       "      <td>23.089050</td>\n",
       "      <td>6.293200</td>\n",
       "      <td>3.403800</td>\n",
       "      <td>13.479600</td>\n",
       "      <td>13.863700</td>\n",
       "      <td>-0.090200</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>6.093700</td>\n",
       "      <td>6.798925</td>\n",
       "      <td>-1.443450</td>\n",
       "      <td>11.885000</td>\n",
       "      <td>1.469100</td>\n",
       "      <td>18.097125</td>\n",
       "      <td>11.844400</td>\n",
       "      <td>7.487725</td>\n",
       "      <td>4.460400</td>\n",
       "      <td>7.542400</td>\n",
       "      <td>13.598925</td>\n",
       "      <td>...</td>\n",
       "      <td>32.622850</td>\n",
       "      <td>5.792000</td>\n",
       "      <td>6.371200</td>\n",
       "      <td>14.623900</td>\n",
       "      <td>1.323600</td>\n",
       "      <td>23.024025</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>1.318725</td>\n",
       "      <td>7.020025</td>\n",
       "      <td>5.905400</td>\n",
       "      <td>3.096400</td>\n",
       "      <td>2.956800</td>\n",
       "      <td>25.907725</td>\n",
       "      <td>3.619900</td>\n",
       "      <td>25.641225</td>\n",
       "      <td>13.745500</td>\n",
       "      <td>2.704400</td>\n",
       "      <td>13.931300</td>\n",
       "      <td>5.338750</td>\n",
       "      <td>4.391125</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>11.011300</td>\n",
       "      <td>7.499175</td>\n",
       "      <td>12.127425</td>\n",
       "      <td>19.456150</td>\n",
       "      <td>-0.590650</td>\n",
       "      <td>11.193800</td>\n",
       "      <td>-1.466000</td>\n",
       "      <td>18.345225</td>\n",
       "      <td>1.482900</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>11.150600</td>\n",
       "      <td>18.670200</td>\n",
       "      <td>17.188700</td>\n",
       "      <td>14.654500</td>\n",
       "      <td>22.331500</td>\n",
       "      <td>14.937700</td>\n",
       "      <td>15.863300</td>\n",
       "      <td>17.950600</td>\n",
       "      <td>19.025900</td>\n",
       "      <td>41.748000</td>\n",
       "      <td>35.183000</td>\n",
       "      <td>31.285900</td>\n",
       "      <td>49.044300</td>\n",
       "      <td>14.594500</td>\n",
       "      <td>4.875200</td>\n",
       "      <td>25.446000</td>\n",
       "      <td>14.654600</td>\n",
       "      <td>15.675100</td>\n",
       "      <td>3.243100</td>\n",
       "      <td>8.787400</td>\n",
       "      <td>13.143100</td>\n",
       "      <td>15.651500</td>\n",
       "      <td>20.171900</td>\n",
       "      <td>6.787100</td>\n",
       "      <td>29.546600</td>\n",
       "      <td>13.287800</td>\n",
       "      <td>21.528900</td>\n",
       "      <td>14.245600</td>\n",
       "      <td>11.863800</td>\n",
       "      <td>29.823500</td>\n",
       "      <td>...</td>\n",
       "      <td>58.394200</td>\n",
       "      <td>6.309900</td>\n",
       "      <td>10.134400</td>\n",
       "      <td>27.564800</td>\n",
       "      <td>12.119300</td>\n",
       "      <td>38.332200</td>\n",
       "      <td>4.220400</td>\n",
       "      <td>21.276600</td>\n",
       "      <td>14.886100</td>\n",
       "      <td>7.089000</td>\n",
       "      <td>16.731900</td>\n",
       "      <td>17.917300</td>\n",
       "      <td>53.591900</td>\n",
       "      <td>18.855400</td>\n",
       "      <td>43.546800</td>\n",
       "      <td>20.854800</td>\n",
       "      <td>20.245200</td>\n",
       "      <td>20.596500</td>\n",
       "      <td>29.841300</td>\n",
       "      <td>13.448700</td>\n",
       "      <td>12.750500</td>\n",
       "      <td>14.393900</td>\n",
       "      <td>29.248700</td>\n",
       "      <td>23.704900</td>\n",
       "      <td>44.363400</td>\n",
       "      <td>12.997500</td>\n",
       "      <td>21.739200</td>\n",
       "      <td>22.786100</td>\n",
       "      <td>29.330300</td>\n",
       "      <td>4.034100</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0  ...        var_198        var_199\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000\n",
       "mean        0.100490      10.679914  ...      15.870720      -3.326537\n",
       "std         0.300653       3.040051  ...       3.010945      10.438015\n",
       "min         0.000000       0.408400  ...       6.299300     -38.852800\n",
       "25%         0.000000       8.453850  ...      13.829700     -11.208475\n",
       "50%         0.000000      10.524750  ...      15.934050      -2.819550\n",
       "75%         0.000000      12.758200  ...      18.064725       4.836800\n",
       "max         1.000000      20.315000  ...      26.079100      28.500700\n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there any null value in train dataset: \" + str(train.isnull().any().any()))\n",
    "print(\"Is there any null value in test dataset: \" + str(test.isnull().any().any()))\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviation Distribution over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "zdU57CtHx2gj",
    "outputId": "60c4ba6d-8a57-456b-95d8-9c8bee475799"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: `Series.plot()` should not be called with positional arguments, only keyword arguments. The order of positional arguments will change in the future. Use `Series.plot(kind='hist')` instead of `Series.plot('hist',)`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'STD Distribution')"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Jekxhj8ktQYg1+SGvP/ASsNuSIhY39iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train[train.columns[2:]].std().plot('hist')\n",
    "plt.title('STD Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Deviation Distribution over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "9BrsmXFmyggr",
    "outputId": "cc763c10-2346-4b55-dd4a-c2f0ddd68fbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: `Series.plot()` should not be called with positional arguments, only keyword arguments. The order of positional arguments will change in the future. Use `Series.plot(kind='hist')` instead of `Series.plot('hist',)`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mean Distribution')"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[train.columns[2:]].mean().plot('hist')\n",
    "plt.title('Mean Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of Target_Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "q3cw_j591V3K",
    "outputId": "8fc5a879-9b58-4f54-c933-87e2a433fe7f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/0AAAHgCAYAAAD36A0MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1296x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax=plt.subplots(1,2,figsize=(18,8))[1]\n",
    "train['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
    "sns.countplot('target',data=train,ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that our data is highly imbalance as only 10% of label is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll try some simple Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "colab_type": "code",
    "id": "IsB3t9pQfaD2",
    "outputId": "62d9b096-06c0-4182-b0ab-06239cdcc43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.5035624394895757\n",
      "Fold: 1\n",
      "CV score: 0.5142348243515059\n",
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.532134012582711\n",
      "Fold: 3\n",
      "CV score: 0.5569476839558692\n",
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.5889860202290409\n",
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.6283853941757361\n",
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.675153393600427\n",
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.7293989338512702\n",
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.7908101529838674\n",
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.8592706869622526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "target = train['target']\n",
    "features = [c for c in train if c not in ['ID_code', 'target']]\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=10, random_state=1).split(train.values, target.values)\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "\n",
    "for k, (trn_idx, val_idx) in enumerate(kfolds):\n",
    "    print('Fold: {}'.format(k))\n",
    "    \n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=2000, C=10)\n",
    "    clf.fit(train.iloc[trn_idx][features], target[trn_idx])\n",
    "    oof[val_idx] = clf.predict_proba(train.iloc[val_idx][features])[:, 1]\n",
    "    predictions += clf.predict_proba(test[features])[:, 1]\n",
    "\n",
    "    print('CV score: {}'.format(roc_auc_score(target.values, oof)))\n",
    "\n",
    "    \n",
    "predictions /= 10\n",
    "    \n",
    "# print('CV score: {}'.format(roc_auc_score(target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression gave us the AUC score of 85.9%, which is not actually bad but let us try some Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "colab_type": "code",
    "id": "yF487ddqqk6W",
    "outputId": "33a38e73-281c-485a-b64d-6f3bdecfbadd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
    "from keras import callbacks\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4CLgyysqo4_"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./sample_data/train.csv', index_col=0)\n",
    "y_train = df_train.pop('target')\n",
    "len_train = len(df_train)\n",
    "df_test = pd.read_csv('./sample_data/test.csv', index_col=0)\n",
    "df_all = pd.concat((df_train, df_test), sort=False)\n",
    "prev_cols = df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jviJIOKrJRC"
   },
   "outputs": [],
   "source": [
    "# PREPROCESS\n",
    "scaler = StandardScaler()\n",
    "df_all[prev_cols] = scaler.fit_transform(df_all[prev_cols])\n",
    "df_train = df_all[0:len_train]\n",
    "df_test = df_all[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KohfFk7xrKuX"
   },
   "outputs": [],
   "source": [
    "# CROSS VALIDATION\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmMQtEhHrNQP"
   },
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "class Logger(callbacks.Callback):\n",
    "    def __init__(self, out_path='./', patience=10, lr_patience=3, out_fn='', log_fn=''):\n",
    "        self.auc = 0\n",
    "        self.path = out_path\n",
    "        self.fn = out_fn\n",
    "        self.patience = patience\n",
    "        self.lr_patience = lr_patience\n",
    "        self.no_improve = 0\n",
    "        self.no_improve_lr = 0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        cv_pred = self.model.predict(self.validation_data[0], batch_size=1024)\n",
    "        cv_true = self.validation_data[1]\n",
    "        auc_val = roc_auc_score(cv_true, cv_pred)\n",
    "        if self.auc < auc_val:\n",
    "            self.no_improve = 0\n",
    "            self.no_improve_lr = 0\n",
    "            print(\"Epoch %s - best AUC: %s\" % (epoch, round(auc_val, 4)))\n",
    "            self.auc = auc_val\n",
    "            self.model.save(self.path + self.fn, overwrite=True)\n",
    "        else:\n",
    "            self.no_improve += 1\n",
    "            self.no_improve_lr += 1\n",
    "            print(\"Epoch %s - current AUC: %s\" % (epoch, round(auc_val, 4)))\n",
    "            if self.no_improve >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "            if self.no_improve_lr >= self.lr_patience:\n",
    "                lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                K.set_value(self.model.optimizer.lr, 0.75*lr)\n",
    "                print(\"Setting lr to {}\".format(0.75*lr))\n",
    "                self.no_improve_lr = 0\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us try simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_30wSEprRRS"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def nn_model():\n",
    "    inp = Input(shape=(200, 1))\n",
    "    d1 = Dense(16, activation='relu')(inp)\n",
    "    fl = Flatten()(d1)\n",
    "    preds = Dense(1, activation='sigmoid')(fl)\n",
    "    model = Model(inputs=inp, outputs=preds)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "7hOZNJ3KrTzp",
    "outputId": "d7302ea6-9ad2-4bc3-a6e5-51684291c2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200, 1)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200, 16)           32        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 3201      \n",
      "=================================================================\n",
      "Total params: 3,233\n",
      "Trainable params: 3,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H4aNtuHNrxIq",
    "outputId": "89c08b06-016f-44b3-d830-a240be78664c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL 0\n",
      "Train on 159999 samples, validate on 40001 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.2595 - acc: 0.9073 - val_loss: 0.2332 - val_acc: 0.9151\n",
      "Epoch 0 - best AUC: 0.8562\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2274 - acc: 0.9153 - val_loss: 0.2306 - val_acc: 0.9153\n",
      "Epoch 1 - best AUC: 0.8696\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.2172 - acc: 0.9189 - val_loss: 0.2174 - val_acc: 0.9207\n",
      "Epoch 2 - best AUC: 0.8778\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.2118 - acc: 0.9204 - val_loss: 0.2147 - val_acc: 0.9207\n",
      "Epoch 3 - best AUC: 0.8811\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.2096 - acc: 0.9212 - val_loss: 0.2180 - val_acc: 0.9195\n",
      "Epoch 4 - best AUC: 0.8827\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2077 - acc: 0.9223 - val_loss: 0.2135 - val_acc: 0.9210\n",
      "Epoch 5 - best AUC: 0.8843\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2067 - acc: 0.9225 - val_loss: 0.2134 - val_acc: 0.9217\n",
      "Epoch 6 - best AUC: 0.8853\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.2053 - acc: 0.9231 - val_loss: 0.2113 - val_acc: 0.9224\n",
      "Epoch 7 - best AUC: 0.886\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.2047 - acc: 0.9231 - val_loss: 0.2101 - val_acc: 0.9227\n",
      "Epoch 8 - best AUC: 0.887\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.2041 - acc: 0.9234 - val_loss: 0.2096 - val_acc: 0.9229\n",
      "Epoch 9 - best AUC: 0.8877\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.2031 - acc: 0.9240 - val_loss: 0.2092 - val_acc: 0.9231\n",
      "Epoch 10 - best AUC: 0.8881\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.2026 - acc: 0.9240 - val_loss: 0.2085 - val_acc: 0.9237\n",
      "Epoch 11 - best AUC: 0.8891\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2023 - acc: 0.9242 - val_loss: 0.2084 - val_acc: 0.9237\n",
      "Epoch 12 - best AUC: 0.8892\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2018 - acc: 0.9243 - val_loss: 0.2081 - val_acc: 0.9238\n",
      "Epoch 13 - best AUC: 0.8897\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2013 - acc: 0.9246 - val_loss: 0.2084 - val_acc: 0.9241\n",
      "Epoch 14 - best AUC: 0.89\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2007 - acc: 0.9246 - val_loss: 0.2084 - val_acc: 0.9238\n",
      "Epoch 15 - best AUC: 0.8902\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.2004 - acc: 0.9249 - val_loss: 0.2086 - val_acc: 0.9240\n",
      "Epoch 16 - best AUC: 0.8905\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.2004 - acc: 0.9248 - val_loss: 0.2106 - val_acc: 0.9234\n",
      "Epoch 17 - best AUC: 0.8906\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2001 - acc: 0.9247 - val_loss: 0.2093 - val_acc: 0.9222\n",
      "Epoch 18 - best AUC: 0.8908\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2002 - acc: 0.9248 - val_loss: 0.2082 - val_acc: 0.9239\n",
      "Epoch 19 - current AUC: 0.8906\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2001 - acc: 0.9248 - val_loss: 0.2076 - val_acc: 0.9241\n",
      "Epoch 20 - best AUC: 0.8909\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.1996 - acc: 0.9248 - val_loss: 0.2071 - val_acc: 0.9234\n",
      "Epoch 21 - best AUC: 0.891\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.1994 - acc: 0.9251 - val_loss: 0.2072 - val_acc: 0.9228\n",
      "Epoch 22 - best AUC: 0.891\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2000 - acc: 0.9247 - val_loss: 0.2078 - val_acc: 0.9240\n",
      "Epoch 23 - best AUC: 0.8911\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.1994 - acc: 0.9250 - val_loss: 0.2092 - val_acc: 0.9219\n",
      "Epoch 24 - current AUC: 0.891\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.1994 - acc: 0.9251 - val_loss: 0.2084 - val_acc: 0.9242\n",
      "Epoch 25 - best AUC: 0.8912\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9253 - val_loss: 0.2086 - val_acc: 0.9222\n",
      "Epoch 26 - best AUC: 0.8913\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9250 - val_loss: 0.2072 - val_acc: 0.9228\n",
      "Epoch 27 - best AUC: 0.8915\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.1989 - acc: 0.9253 - val_loss: 0.2076 - val_acc: 0.9229\n",
      "Epoch 28 - current AUC: 0.8912\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.1989 - acc: 0.9254 - val_loss: 0.2068 - val_acc: 0.9237\n",
      "Epoch 29 - current AUC: 0.8914\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.1986 - acc: 0.9253 - val_loss: 0.2075 - val_acc: 0.9229\n",
      "Epoch 30 - best AUC: 0.8915\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.1989 - acc: 0.9254 - val_loss: 0.2079 - val_acc: 0.9237\n",
      "Epoch 31 - current AUC: 0.8914\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9252 - val_loss: 0.2070 - val_acc: 0.9233\n",
      "Epoch 32 - current AUC: 0.8915\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9253 - val_loss: 0.2076 - val_acc: 0.9243\n",
      "Epoch 33 - current AUC: 0.8914\n",
      "Setting lr to 0.0007500000356230885\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9254 - val_loss: 0.2072 - val_acc: 0.9240\n",
      "Epoch 34 - current AUC: 0.8914\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9256 - val_loss: 0.2075 - val_acc: 0.9230\n",
      "Epoch 35 - current AUC: 0.8915\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9253 - val_loss: 0.2068 - val_acc: 0.9234\n",
      "Epoch 36 - current AUC: 0.8915\n",
      "Setting lr to 0.0005625000048894435\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9258 - val_loss: 0.2067 - val_acc: 0.9235\n",
      "Epoch 37 - best AUC: 0.8916\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.1974 - acc: 0.9258 - val_loss: 0.2068 - val_acc: 0.9233\n",
      "Epoch 38 - current AUC: 0.8916\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9259 - val_loss: 0.2076 - val_acc: 0.9242\n",
      "Epoch 39 - current AUC: 0.8915\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.1973 - acc: 0.9258 - val_loss: 0.2068 - val_acc: 0.9235\n",
      "Epoch 40 - best AUC: 0.8917\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.1973 - acc: 0.9258 - val_loss: 0.2086 - val_acc: 0.9224\n",
      "Epoch 41 - current AUC: 0.8916\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9258 - val_loss: 0.2068 - val_acc: 0.9236\n",
      "Epoch 42 - current AUC: 0.8917\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9256 - val_loss: 0.2075 - val_acc: 0.9239\n",
      "Epoch 43 - current AUC: 0.8917\n",
      "Setting lr to 0.0004218749818392098\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.1969 - acc: 0.9260 - val_loss: 0.2067 - val_acc: 0.9235\n",
      "Epoch 44 - best AUC: 0.8918\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.1970 - acc: 0.9260 - val_loss: 0.2066 - val_acc: 0.9232\n",
      "Epoch 45 - current AUC: 0.8918\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.1968 - acc: 0.9262 - val_loss: 0.2066 - val_acc: 0.9237\n",
      "Epoch 46 - current AUC: 0.8918\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.1968 - acc: 0.9262 - val_loss: 0.2069 - val_acc: 0.9238\n",
      "Epoch 47 - current AUC: 0.8917\n",
      "Setting lr to 0.00031640623637940735\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.1966 - acc: 0.9259 - val_loss: 0.2068 - val_acc: 0.9240\n",
      "Epoch 48 - current AUC: 0.8918\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.1964 - acc: 0.9263 - val_loss: 0.2068 - val_acc: 0.9240\n",
      "Epoch 49 - current AUC: 0.8917\n",
      "VAL 1\n",
      "Train on 159999 samples, validate on 40001 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.2626 - acc: 0.9055 - val_loss: 0.2338 - val_acc: 0.9132\n",
      "Epoch 0 - best AUC: 0.8602\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2312 - acc: 0.9146 - val_loss: 0.2268 - val_acc: 0.9154\n",
      "Epoch 1 - best AUC: 0.8681\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.2227 - acc: 0.9174 - val_loss: 0.2183 - val_acc: 0.9186\n",
      "Epoch 2 - best AUC: 0.8841\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.2134 - acc: 0.9204 - val_loss: 0.2098 - val_acc: 0.9212\n",
      "Epoch 3 - best AUC: 0.8912\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.2097 - acc: 0.9215 - val_loss: 0.2127 - val_acc: 0.9202\n",
      "Epoch 4 - best AUC: 0.8936\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2080 - acc: 0.9227 - val_loss: 0.2070 - val_acc: 0.9223\n",
      "Epoch 5 - best AUC: 0.8957\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2067 - acc: 0.9233 - val_loss: 0.2082 - val_acc: 0.9219\n",
      "Epoch 6 - best AUC: 0.8963\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.2060 - acc: 0.9232 - val_loss: 0.2043 - val_acc: 0.9228\n",
      "Epoch 7 - best AUC: 0.8976\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.2042 - acc: 0.9244 - val_loss: 0.2038 - val_acc: 0.9231\n",
      "Epoch 8 - best AUC: 0.8979\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.2041 - acc: 0.9241 - val_loss: 0.2036 - val_acc: 0.9233\n",
      "Epoch 9 - best AUC: 0.8984\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.2036 - acc: 0.9243 - val_loss: 0.2031 - val_acc: 0.9233\n",
      "Epoch 10 - best AUC: 0.8991\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.2030 - acc: 0.9248 - val_loss: 0.2062 - val_acc: 0.9225\n",
      "Epoch 11 - best AUC: 0.8996\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2027 - acc: 0.9247 - val_loss: 0.2027 - val_acc: 0.9232\n",
      "Epoch 12 - best AUC: 0.8997\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2022 - acc: 0.9249 - val_loss: 0.2031 - val_acc: 0.9229\n",
      "Epoch 13 - best AUC: 0.8997\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2016 - acc: 0.9249 - val_loss: 0.2034 - val_acc: 0.9227\n",
      "Epoch 14 - best AUC: 0.8998\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2017 - acc: 0.9249 - val_loss: 0.2024 - val_acc: 0.9228\n",
      "Epoch 15 - best AUC: 0.9002\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.2016 - acc: 0.9250 - val_loss: 0.2026 - val_acc: 0.9231\n",
      "Epoch 16 - current AUC: 0.9001\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2012 - acc: 0.9251 - val_loss: 0.2063 - val_acc: 0.9210\n",
      "Epoch 17 - best AUC: 0.9003\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2011 - acc: 0.9251 - val_loss: 0.2027 - val_acc: 0.9237\n",
      "Epoch 18 - current AUC: 0.9001\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2011 - acc: 0.9251 - val_loss: 0.2025 - val_acc: 0.9228\n",
      "Epoch 19 - best AUC: 0.9007\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2009 - acc: 0.9251 - val_loss: 0.2024 - val_acc: 0.9229\n",
      "Epoch 20 - current AUC: 0.9005\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2009 - acc: 0.9251 - val_loss: 0.2025 - val_acc: 0.9230\n",
      "Epoch 21 - current AUC: 0.9005\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2010 - acc: 0.9254 - val_loss: 0.2031 - val_acc: 0.9222\n",
      "Epoch 22 - current AUC: 0.9001\n",
      "Setting lr to 0.0007500000356230885\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.1998 - acc: 0.9257 - val_loss: 0.2023 - val_acc: 0.9224\n",
      "Epoch 23 - best AUC: 0.9008\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2000 - acc: 0.9257 - val_loss: 0.2040 - val_acc: 0.9217\n",
      "Epoch 24 - current AUC: 0.9007\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.1999 - acc: 0.9256 - val_loss: 0.2022 - val_acc: 0.9233\n",
      "Epoch 25 - current AUC: 0.9006\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.1998 - acc: 0.9256 - val_loss: 0.2030 - val_acc: 0.9221\n",
      "Epoch 26 - current AUC: 0.9006\n",
      "Setting lr to 0.0005625000048894435\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.1993 - acc: 0.9259 - val_loss: 0.2021 - val_acc: 0.9230\n",
      "Epoch 27 - current AUC: 0.9007\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9257 - val_loss: 0.2031 - val_acc: 0.9221\n",
      "Epoch 28 - current AUC: 0.9007\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9258 - val_loss: 0.2021 - val_acc: 0.9231\n",
      "Epoch 29 - current AUC: 0.9007\n",
      "Setting lr to 0.0004218749818392098\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.1986 - acc: 0.9259 - val_loss: 0.2020 - val_acc: 0.9229\n",
      "Epoch 30 - best AUC: 0.9008\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9261 - val_loss: 0.2020 - val_acc: 0.9230\n",
      "Epoch 31 - best AUC: 0.9008\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9260 - val_loss: 0.2022 - val_acc: 0.9228\n",
      "Epoch 32 - best AUC: 0.9008\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9261 - val_loss: 0.2021 - val_acc: 0.9229\n",
      "Epoch 33 - current AUC: 0.9007\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9259 - val_loss: 0.2029 - val_acc: 0.9234\n",
      "Epoch 34 - current AUC: 0.9007\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9261 - val_loss: 0.2020 - val_acc: 0.9230\n",
      "Epoch 35 - current AUC: 0.9007\n",
      "Setting lr to 0.00031640623637940735\n",
      "Epoch 37/50\n",
      " - 4s - loss: 0.1982 - acc: 0.9263 - val_loss: 0.2025 - val_acc: 0.9227\n",
      "Epoch 36 - best AUC: 0.9009\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.1982 - acc: 0.9262 - val_loss: 0.2019 - val_acc: 0.9230\n",
      "Epoch 37 - current AUC: 0.9008\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9262 - val_loss: 0.2019 - val_acc: 0.9233\n",
      "Epoch 38 - current AUC: 0.9008\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9263 - val_loss: 0.2019 - val_acc: 0.9232\n",
      "Epoch 39 - current AUC: 0.9008\n",
      "Setting lr to 0.00023730468819849193\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.1978 - acc: 0.9264 - val_loss: 0.2019 - val_acc: 0.9231\n",
      "Epoch 40 - current AUC: 0.9008\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.1978 - acc: 0.9263 - val_loss: 0.2019 - val_acc: 0.9232\n",
      "Epoch 41 - current AUC: 0.9008\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.1977 - acc: 0.9263 - val_loss: 0.2029 - val_acc: 0.9217\n",
      "Epoch 42 - current AUC: 0.9009\n",
      "Setting lr to 0.00017797851614886895\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9264 - val_loss: 0.2019 - val_acc: 0.9229\n",
      "Epoch 43 - best AUC: 0.9009\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9264 - val_loss: 0.2018 - val_acc: 0.9233\n",
      "Epoch 44 - best AUC: 0.9009\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9264 - val_loss: 0.2019 - val_acc: 0.9231\n",
      "Epoch 45 - current AUC: 0.9009\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9265 - val_loss: 0.2022 - val_acc: 0.9228\n",
      "Epoch 46 - current AUC: 0.9008\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9264 - val_loss: 0.2019 - val_acc: 0.9230\n",
      "Epoch 47 - current AUC: 0.9009\n",
      "Setting lr to 0.0001334838816546835\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.1973 - acc: 0.9266 - val_loss: 0.2019 - val_acc: 0.9232\n",
      "Epoch 48 - current AUC: 0.9009\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.1974 - acc: 0.9264 - val_loss: 0.2024 - val_acc: 0.9221\n",
      "Epoch 49 - current AUC: 0.9009\n",
      "VAL 2\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.2561 - acc: 0.9081 - val_loss: 0.2336 - val_acc: 0.9133\n",
      "Epoch 0 - best AUC: 0.8565\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2301 - acc: 0.9147 - val_loss: 0.2270 - val_acc: 0.9158\n",
      "Epoch 1 - best AUC: 0.8661\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.2205 - acc: 0.9181 - val_loss: 0.2171 - val_acc: 0.9195\n",
      "Epoch 2 - best AUC: 0.8811\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.2123 - acc: 0.9207 - val_loss: 0.2121 - val_acc: 0.9211\n",
      "Epoch 3 - best AUC: 0.8878\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2092 - acc: 0.9220 - val_loss: 0.2088 - val_acc: 0.9224\n",
      "Epoch 4 - best AUC: 0.8901\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2074 - acc: 0.9226 - val_loss: 0.2086 - val_acc: 0.9224\n",
      "Epoch 5 - best AUC: 0.8915\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2058 - acc: 0.9229 - val_loss: 0.2081 - val_acc: 0.9233\n",
      "Epoch 6 - current AUC: 0.8914\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.2056 - acc: 0.9231 - val_loss: 0.2188 - val_acc: 0.9195\n",
      "Epoch 7 - best AUC: 0.8927\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.2048 - acc: 0.9234 - val_loss: 0.2060 - val_acc: 0.9238\n",
      "Epoch 8 - best AUC: 0.8931\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.2034 - acc: 0.9242 - val_loss: 0.2072 - val_acc: 0.9230\n",
      "Epoch 9 - best AUC: 0.8935\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.2030 - acc: 0.9242 - val_loss: 0.2052 - val_acc: 0.9234\n",
      "Epoch 10 - best AUC: 0.8939\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.2023 - acc: 0.9247 - val_loss: 0.2052 - val_acc: 0.9239\n",
      "Epoch 11 - best AUC: 0.8941\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2022 - acc: 0.9245 - val_loss: 0.2048 - val_acc: 0.9239\n",
      "Epoch 12 - best AUC: 0.8943\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2019 - acc: 0.9244 - val_loss: 0.2053 - val_acc: 0.9241\n",
      "Epoch 13 - current AUC: 0.8943\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2015 - acc: 0.9241 - val_loss: 0.2045 - val_acc: 0.9244\n",
      "Epoch 14 - best AUC: 0.8947\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2013 - acc: 0.9244 - val_loss: 0.2043 - val_acc: 0.9242\n",
      "Epoch 15 - best AUC: 0.8949\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.2007 - acc: 0.9250 - val_loss: 0.2061 - val_acc: 0.9237\n",
      "Epoch 16 - current AUC: 0.8948\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2008 - acc: 0.9248 - val_loss: 0.2051 - val_acc: 0.9241\n",
      "Epoch 17 - best AUC: 0.8949\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2008 - acc: 0.9248 - val_loss: 0.2041 - val_acc: 0.9245\n",
      "Epoch 18 - best AUC: 0.8951\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2004 - acc: 0.9247 - val_loss: 0.2065 - val_acc: 0.9232\n",
      "Epoch 19 - current AUC: 0.8947\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2004 - acc: 0.9249 - val_loss: 0.2042 - val_acc: 0.9245\n",
      "Epoch 20 - best AUC: 0.8952\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2003 - acc: 0.9248 - val_loss: 0.2047 - val_acc: 0.9236\n",
      "Epoch 21 - current AUC: 0.8951\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2005 - acc: 0.9252 - val_loss: 0.2054 - val_acc: 0.9235\n",
      "Epoch 22 - current AUC: 0.8951\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.1998 - acc: 0.9250 - val_loss: 0.2042 - val_acc: 0.9243\n",
      "Epoch 23 - current AUC: 0.8951\n",
      "Setting lr to 0.0007500000356230885\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.1996 - acc: 0.9249 - val_loss: 0.2059 - val_acc: 0.9229\n",
      "Epoch 24 - best AUC: 0.8953\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9250 - val_loss: 0.2047 - val_acc: 0.9240\n",
      "Epoch 25 - current AUC: 0.8951\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9253 - val_loss: 0.2042 - val_acc: 0.9244\n",
      "Epoch 26 - current AUC: 0.8952\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9250 - val_loss: 0.2043 - val_acc: 0.9244\n",
      "Epoch 27 - current AUC: 0.895\n",
      "Setting lr to 0.0005625000048894435\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9252 - val_loss: 0.2049 - val_acc: 0.9237\n",
      "Epoch 28 - current AUC: 0.895\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9256 - val_loss: 0.2044 - val_acc: 0.9241\n",
      "Epoch 29 - current AUC: 0.8951\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9254 - val_loss: 0.2050 - val_acc: 0.9234\n",
      "Epoch 30 - best AUC: 0.8953\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.1985 - acc: 0.9254 - val_loss: 0.2056 - val_acc: 0.9241\n",
      "Epoch 31 - current AUC: 0.8951\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.1984 - acc: 0.9254 - val_loss: 0.2045 - val_acc: 0.9238\n",
      "Epoch 32 - current AUC: 0.8953\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.1984 - acc: 0.9256 - val_loss: 0.2045 - val_acc: 0.9239\n",
      "Epoch 33 - current AUC: 0.8952\n",
      "Setting lr to 0.0004218749818392098\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9257 - val_loss: 0.2043 - val_acc: 0.9242\n",
      "Epoch 34 - current AUC: 0.8953\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9257 - val_loss: 0.2044 - val_acc: 0.9245\n",
      "Epoch 35 - current AUC: 0.8952\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.1979 - acc: 0.9256 - val_loss: 0.2043 - val_acc: 0.9243\n",
      "Epoch 36 - current AUC: 0.8953\n",
      "Setting lr to 0.00031640623637940735\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9256 - val_loss: 0.2042 - val_acc: 0.9243\n",
      "Epoch 37 - best AUC: 0.8953\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9258 - val_loss: 0.2045 - val_acc: 0.9239\n",
      "Epoch 38 - best AUC: 0.8954\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9257 - val_loss: 0.2047 - val_acc: 0.9242\n",
      "Epoch 39 - best AUC: 0.8954\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9254 - val_loss: 0.2044 - val_acc: 0.9242\n",
      "Epoch 40 - current AUC: 0.8953\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.1974 - acc: 0.9257 - val_loss: 0.2054 - val_acc: 0.9240\n",
      "Epoch 41 - current AUC: 0.8954\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.1974 - acc: 0.9258 - val_loss: 0.2045 - val_acc: 0.9241\n",
      "Epoch 42 - current AUC: 0.8953\n",
      "Setting lr to 0.00023730468819849193\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9258 - val_loss: 0.2045 - val_acc: 0.9241\n",
      "Epoch 43 - current AUC: 0.8954\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9260 - val_loss: 0.2043 - val_acc: 0.9240\n",
      "Epoch 44 - current AUC: 0.8953\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.1972 - acc: 0.9259 - val_loss: 0.2049 - val_acc: 0.9241\n",
      "Epoch 45 - current AUC: 0.8954\n",
      "Setting lr to 0.00017797851614886895\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.1970 - acc: 0.9257 - val_loss: 0.2049 - val_acc: 0.9235\n",
      "Epoch 46 - current AUC: 0.8954\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.1969 - acc: 0.9260 - val_loss: 0.2042 - val_acc: 0.9245\n",
      "Epoch 47 - current AUC: 0.8954\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.1970 - acc: 0.9260 - val_loss: 0.2041 - val_acc: 0.9245\n",
      "Epoch 48 - current AUC: 0.8954\n",
      "Setting lr to 0.0001334838816546835\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.1967 - acc: 0.9261 - val_loss: 0.2042 - val_acc: 0.9244\n",
      "Epoch 49 - current AUC: 0.8954\n",
      "VAL 3\n",
      "Train on 160001 samples, validate on 39999 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.2601 - acc: 0.9062 - val_loss: 0.2341 - val_acc: 0.9131\n",
      "Epoch 0 - best AUC: 0.8601\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2306 - acc: 0.9150 - val_loss: 0.2250 - val_acc: 0.9158\n",
      "Epoch 1 - best AUC: 0.8722\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.2212 - acc: 0.9180 - val_loss: 0.2126 - val_acc: 0.9208\n",
      "Epoch 2 - best AUC: 0.8869\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.2143 - acc: 0.9200 - val_loss: 0.2323 - val_acc: 0.9159\n",
      "Epoch 3 - best AUC: 0.8926\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.2139 - acc: 0.9206 - val_loss: 0.2141 - val_acc: 0.9222\n",
      "Epoch 4 - best AUC: 0.8942\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2124 - acc: 0.9209 - val_loss: 0.2062 - val_acc: 0.9236\n",
      "Epoch 5 - best AUC: 0.8957\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2083 - acc: 0.9225 - val_loss: 0.2036 - val_acc: 0.9229\n",
      "Epoch 6 - best AUC: 0.8973\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.2075 - acc: 0.9228 - val_loss: 0.2033 - val_acc: 0.9236\n",
      "Epoch 7 - best AUC: 0.8981\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.2068 - acc: 0.9228 - val_loss: 0.2029 - val_acc: 0.9232\n",
      "Epoch 8 - best AUC: 0.8986\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.2063 - acc: 0.9229 - val_loss: 0.2066 - val_acc: 0.9224\n",
      "Epoch 9 - best AUC: 0.8996\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.2053 - acc: 0.9236 - val_loss: 0.2013 - val_acc: 0.9243\n",
      "Epoch 10 - best AUC: 0.9001\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.2047 - acc: 0.9236 - val_loss: 0.2041 - val_acc: 0.9230\n",
      "Epoch 11 - best AUC: 0.9007\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2045 - acc: 0.9241 - val_loss: 0.2017 - val_acc: 0.9250\n",
      "Epoch 12 - best AUC: 0.9012\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2042 - acc: 0.9240 - val_loss: 0.1998 - val_acc: 0.9250\n",
      "Epoch 13 - best AUC: 0.9016\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2034 - acc: 0.9243 - val_loss: 0.2223 - val_acc: 0.9165\n",
      "Epoch 14 - best AUC: 0.9017\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2053 - acc: 0.9234 - val_loss: 0.2009 - val_acc: 0.9246\n",
      "Epoch 15 - current AUC: 0.9011\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.2024 - acc: 0.9244 - val_loss: 0.1998 - val_acc: 0.9249\n",
      "Epoch 16 - best AUC: 0.9019\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2026 - acc: 0.9244 - val_loss: 0.1996 - val_acc: 0.9251\n",
      "Epoch 17 - best AUC: 0.902\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2022 - acc: 0.9244 - val_loss: 0.1994 - val_acc: 0.9256\n",
      "Epoch 18 - best AUC: 0.9023\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2019 - acc: 0.9247 - val_loss: 0.2007 - val_acc: 0.9244\n",
      "Epoch 19 - best AUC: 0.9028\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2021 - acc: 0.9243 - val_loss: 0.1990 - val_acc: 0.9250\n",
      "Epoch 20 - best AUC: 0.9028\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2019 - acc: 0.9247 - val_loss: 0.1988 - val_acc: 0.9254\n",
      "Epoch 21 - best AUC: 0.9033\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.2016 - acc: 0.9244 - val_loss: 0.1991 - val_acc: 0.9253\n",
      "Epoch 22 - current AUC: 0.9031\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2017 - acc: 0.9242 - val_loss: 0.2068 - val_acc: 0.9226\n",
      "Epoch 23 - current AUC: 0.9029\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2026 - acc: 0.9247 - val_loss: 0.1992 - val_acc: 0.9253\n",
      "Epoch 24 - current AUC: 0.9032\n",
      "Setting lr to 0.0007500000356230885\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.2008 - acc: 0.9249 - val_loss: 0.1985 - val_acc: 0.9253\n",
      "Epoch 25 - best AUC: 0.9034\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.2006 - acc: 0.9249 - val_loss: 0.1986 - val_acc: 0.9253\n",
      "Epoch 26 - current AUC: 0.9034\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.2005 - acc: 0.9249 - val_loss: 0.2089 - val_acc: 0.9223\n",
      "Epoch 27 - best AUC: 0.9035\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.2019 - acc: 0.9244 - val_loss: 0.1998 - val_acc: 0.9254\n",
      "Epoch 28 - current AUC: 0.9028\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.2007 - acc: 0.9250 - val_loss: 0.1989 - val_acc: 0.9255\n",
      "Epoch 29 - current AUC: 0.9032\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.2002 - acc: 0.9251 - val_loss: 0.1986 - val_acc: 0.9259\n",
      "Epoch 30 - current AUC: 0.9035\n",
      "Setting lr to 0.0005625000048894435\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.1996 - acc: 0.9250 - val_loss: 0.1985 - val_acc: 0.9258\n",
      "Epoch 31 - current AUC: 0.9035\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.1999 - acc: 0.9255 - val_loss: 0.1988 - val_acc: 0.9255\n",
      "Epoch 32 - best AUC: 0.9035\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.1999 - acc: 0.9252 - val_loss: 0.1985 - val_acc: 0.9256\n",
      "Epoch 33 - current AUC: 0.9034\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.1997 - acc: 0.9251 - val_loss: 0.1984 - val_acc: 0.9255\n",
      "Epoch 34 - best AUC: 0.9036\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.1997 - acc: 0.9252 - val_loss: 0.1997 - val_acc: 0.9254\n",
      "Epoch 35 - current AUC: 0.9035\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.1996 - acc: 0.9253 - val_loss: 0.1991 - val_acc: 0.9253\n",
      "Epoch 36 - current AUC: 0.9035\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.1996 - acc: 0.9251 - val_loss: 0.1988 - val_acc: 0.9257\n",
      "Epoch 37 - current AUC: 0.9034\n",
      "Setting lr to 0.0004218749818392098\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9255 - val_loss: 0.1991 - val_acc: 0.9255\n",
      "Epoch 38 - current AUC: 0.9035\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.1993 - acc: 0.9253 - val_loss: 0.1993 - val_acc: 0.9252\n",
      "Epoch 39 - current AUC: 0.9034\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9254 - val_loss: 0.2001 - val_acc: 0.9254\n",
      "Epoch 40 - current AUC: 0.9034\n",
      "Setting lr to 0.00031640623637940735\n",
      "Epoch 42/50\n",
      " - 4s - loss: 0.1991 - acc: 0.9255 - val_loss: 0.1988 - val_acc: 0.9257\n",
      "Epoch 41 - current AUC: 0.9032\n",
      "Epoch 43/50\n",
      " - 4s - loss: 0.1988 - acc: 0.9256 - val_loss: 0.1986 - val_acc: 0.9258\n",
      "Epoch 42 - current AUC: 0.9033\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.1988 - acc: 0.9254 - val_loss: 0.1989 - val_acc: 0.9253\n",
      "Epoch 43 - current AUC: 0.9034\n",
      "Setting lr to 0.00023730468819849193\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9254 - val_loss: 0.1987 - val_acc: 0.9253\n",
      "Epoch 44 - current AUC: 0.9033\n",
      "VAL 4\n",
      "Train on 160001 samples, validate on 39999 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.2558 - acc: 0.9086 - val_loss: 0.2335 - val_acc: 0.9139\n",
      "Epoch 0 - best AUC: 0.859\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.2307 - acc: 0.9150 - val_loss: 0.2368 - val_acc: 0.9133\n",
      "Epoch 1 - best AUC: 0.8687\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.2264 - acc: 0.9162 - val_loss: 0.2216 - val_acc: 0.9176\n",
      "Epoch 2 - best AUC: 0.8757\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.2163 - acc: 0.9196 - val_loss: 0.2337 - val_acc: 0.9141\n",
      "Epoch 3 - best AUC: 0.8871\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.2145 - acc: 0.9201 - val_loss: 0.2115 - val_acc: 0.9206\n",
      "Epoch 4 - best AUC: 0.8895\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2085 - acc: 0.9222 - val_loss: 0.2298 - val_acc: 0.9138\n",
      "Epoch 5 - best AUC: 0.8919\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2080 - acc: 0.9224 - val_loss: 0.2209 - val_acc: 0.9177\n",
      "Epoch 6 - best AUC: 0.8922\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.2061 - acc: 0.9233 - val_loss: 0.2083 - val_acc: 0.9216\n",
      "Epoch 7 - best AUC: 0.8939\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.2049 - acc: 0.9241 - val_loss: 0.2066 - val_acc: 0.9225\n",
      "Epoch 8 - best AUC: 0.8949\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.2043 - acc: 0.9237 - val_loss: 0.2114 - val_acc: 0.9209\n",
      "Epoch 9 - best AUC: 0.8954\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.2043 - acc: 0.9239 - val_loss: 0.2087 - val_acc: 0.9224\n",
      "Epoch 10 - best AUC: 0.8958\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.2038 - acc: 0.9241 - val_loss: 0.2078 - val_acc: 0.9219\n",
      "Epoch 11 - best AUC: 0.8958\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2030 - acc: 0.9243 - val_loss: 0.2054 - val_acc: 0.9227\n",
      "Epoch 12 - best AUC: 0.8961\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2024 - acc: 0.9243 - val_loss: 0.2049 - val_acc: 0.9230\n",
      "Epoch 13 - best AUC: 0.897\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2022 - acc: 0.9247 - val_loss: 0.2078 - val_acc: 0.9220\n",
      "Epoch 14 - best AUC: 0.8972\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.2021 - acc: 0.9246 - val_loss: 0.2060 - val_acc: 0.9227\n",
      "Epoch 15 - current AUC: 0.8971\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.2016 - acc: 0.9248 - val_loss: 0.2045 - val_acc: 0.9230\n",
      "Epoch 16 - best AUC: 0.8973\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.2017 - acc: 0.9249 - val_loss: 0.2046 - val_acc: 0.9229\n",
      "Epoch 17 - best AUC: 0.8975\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.2013 - acc: 0.9246 - val_loss: 0.2075 - val_acc: 0.9219\n",
      "Epoch 18 - best AUC: 0.8978\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.2009 - acc: 0.9249 - val_loss: 0.2196 - val_acc: 0.9185\n",
      "Epoch 19 - current AUC: 0.8976\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.2021 - acc: 0.9246 - val_loss: 0.2044 - val_acc: 0.9232\n",
      "Epoch 20 - best AUC: 0.8979\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.2008 - acc: 0.9249 - val_loss: 0.2041 - val_acc: 0.9239\n",
      "Epoch 21 - best AUC: 0.898\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.2006 - acc: 0.9248 - val_loss: 0.2054 - val_acc: 0.9231\n",
      "Epoch 22 - current AUC: 0.898\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.2003 - acc: 0.9249 - val_loss: 0.2039 - val_acc: 0.9237\n",
      "Epoch 23 - best AUC: 0.8985\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.2005 - acc: 0.9248 - val_loss: 0.2044 - val_acc: 0.9232\n",
      "Epoch 24 - current AUC: 0.8984\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.2004 - acc: 0.9247 - val_loss: 0.2041 - val_acc: 0.9233\n",
      "Epoch 25 - current AUC: 0.898\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.2001 - acc: 0.9252 - val_loss: 0.2037 - val_acc: 0.9240\n",
      "Epoch 26 - current AUC: 0.8984\n",
      "Setting lr to 0.0007500000356230885\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.1994 - acc: 0.9254 - val_loss: 0.2045 - val_acc: 0.9229\n",
      "Epoch 27 - current AUC: 0.8985\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.1997 - acc: 0.9256 - val_loss: 0.2095 - val_acc: 0.9215\n",
      "Epoch 28 - best AUC: 0.8986\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.1994 - acc: 0.9254 - val_loss: 0.2037 - val_acc: 0.9240\n",
      "Epoch 29 - best AUC: 0.8986\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.1993 - acc: 0.9252 - val_loss: 0.2045 - val_acc: 0.9233\n",
      "Epoch 30 - best AUC: 0.8988\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.1992 - acc: 0.9252 - val_loss: 0.2037 - val_acc: 0.9236\n",
      "Epoch 31 - current AUC: 0.8988\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.1993 - acc: 0.9256 - val_loss: 0.2042 - val_acc: 0.9233\n",
      "Epoch 32 - current AUC: 0.8986\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.1991 - acc: 0.9254 - val_loss: 0.2045 - val_acc: 0.9235\n",
      "Epoch 33 - current AUC: 0.8985\n",
      "Setting lr to 0.0005625000048894435\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.1987 - acc: 0.9254 - val_loss: 0.2038 - val_acc: 0.9236\n",
      "Epoch 34 - current AUC: 0.8987\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.1993 - acc: 0.9252 - val_loss: 0.2054 - val_acc: 0.9230\n",
      "Epoch 35 - current AUC: 0.8985\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.1984 - acc: 0.9256 - val_loss: 0.2064 - val_acc: 0.9226\n",
      "Epoch 36 - current AUC: 0.8988\n",
      "Setting lr to 0.0004218749818392098\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.1981 - acc: 0.9257 - val_loss: 0.2036 - val_acc: 0.9237\n",
      "Epoch 37 - best AUC: 0.8989\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9259 - val_loss: 0.2037 - val_acc: 0.9236\n",
      "Epoch 38 - best AUC: 0.8989\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9257 - val_loss: 0.2034 - val_acc: 0.9239\n",
      "Epoch 39 - best AUC: 0.899\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9256 - val_loss: 0.2041 - val_acc: 0.9236\n",
      "Epoch 40 - best AUC: 0.899\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.1980 - acc: 0.9257 - val_loss: 0.2039 - val_acc: 0.9237\n",
      "Epoch 41 - current AUC: 0.899\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.1979 - acc: 0.9258 - val_loss: 0.2034 - val_acc: 0.9240\n",
      "Epoch 42 - current AUC: 0.8989\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.1979 - acc: 0.9259 - val_loss: 0.2038 - val_acc: 0.9236\n",
      "Epoch 43 - current AUC: 0.8988\n",
      "Setting lr to 0.00031640623637940735\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9257 - val_loss: 0.2038 - val_acc: 0.9238\n",
      "Epoch 44 - current AUC: 0.8988\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.1975 - acc: 0.9259 - val_loss: 0.2038 - val_acc: 0.9238\n",
      "Epoch 45 - current AUC: 0.8989\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.1976 - acc: 0.9259 - val_loss: 0.2036 - val_acc: 0.9236\n",
      "Epoch 46 - current AUC: 0.8989\n",
      "Setting lr to 0.00023730468819849193\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.1973 - acc: 0.9260 - val_loss: 0.2036 - val_acc: 0.9239\n",
      "Epoch 47 - current AUC: 0.8989\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.1974 - acc: 0.9260 - val_loss: 0.2039 - val_acc: 0.9236\n",
      "Epoch 48 - current AUC: 0.8989\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.1973 - acc: 0.9261 - val_loss: 0.2034 - val_acc: 0.9239\n",
      "Epoch 49 - current AUC: 0.899\n",
      "Setting lr to 0.00017797851614886895\n",
      "CV_AUC: 0.8979279976935154\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "c = 0\n",
    "oof_preds = np.zeros((len(df_train), 1))\n",
    "for train, valid in cv.split(df_train, y_train):\n",
    "    print(\"VAL %s\" % c)\n",
    "    X_train = np.reshape(df_train.iloc[train].values, (-1, 200, 1))\n",
    "    y_train_ = y_train.iloc[train].values\n",
    "    X_valid = np.reshape(df_train.iloc[valid].values, (-1, 200, 1))\n",
    "    y_valid = y_train.iloc[valid].values\n",
    "    model = nn_model()\n",
    "    logger = Logger(patience=10, out_path='./', out_fn='cv_{}.h5'.format(c))\n",
    "    model.fit(X_train, y_train_, validation_data=(X_valid, y_valid), epochs=50, verbose=2, batch_size=256,\n",
    "              callbacks=[logger])\n",
    "    model.load_weights('cv_{}.h5'.format(c))\n",
    "    X_test = np.reshape(df_test.values, (200000, 200, 1))\n",
    "    curr_preds = model.predict(X_test, batch_size=2048)\n",
    "    oof_preds[valid] = model.predict(X_valid)\n",
    "    preds.append(curr_preds)\n",
    "    c += 1\n",
    "auc = roc_auc_score(y_train, oof_preds)\n",
    "print(\"CV_AUC: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwoHWEwCE5Vf"
   },
   "outputs": [],
   "source": [
    "preds = np.asarray(preds)\n",
    "preds = preds.reshape((5, 200000))\n",
    "preds_final = np.mean(preds.T, axis=1)\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = preds_final\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple neural network gave us AUC score of 89.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll try Convolution Neural Network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "9ptMxsFLGZup",
    "outputId": "9fe2e928-4c30-4b0e-c304-afc234c6a6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 real test\n",
      "Found 100000 fake test\n",
      "Loaded 200000 rows of train\n",
      "Loaded 200000 rows of test\n",
      "Found 100000 real test\n",
      "Found 100000 fake test\n",
      "Loaded 200000 rows of train\n",
      "Loaded 200000 rows of test\n"
     ]
    }
   ],
   "source": [
    "train_path = './sample_data/train.csv'\n",
    "test_path = './sample_data/test.csv'\n",
    "\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test.drop(['ID_code'], axis=1, inplace=True)\n",
    "df_test = df_test.values\n",
    "\n",
    "unique_samples = []\n",
    "unique_count = np.zeros_like(df_test)\n",
    "for feature in range(df_test.shape[1]):\n",
    "    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index_[count_ == 1], feature] += 1\n",
    "\n",
    "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "print('Found',len(real_samples_indexes),'real test')\n",
    "print('Found',len(synthetic_samples_indexes),'fake test')\n",
    "\n",
    "\n",
    "d = {}\n",
    "for i in range(200): d['var_'+str(i)] = 'float32'\n",
    "d['target'] = 'uint8'\n",
    "d['ID_code'] = 'object'\n",
    "\n",
    "train = pd.read_csv('./sample_data/train.csv', dtype=d)\n",
    "test = pd.read_csv('./sample_data/test.csv', dtype=d)\n",
    "\n",
    "print('Loaded',len(train),'rows of train')\n",
    "print('Loaded',len(test),'rows of test')\n",
    "print('Found',len(real_samples_indexes),'real test')\n",
    "print('Found',len(synthetic_samples_indexes),'fake test')\n",
    "\n",
    "\n",
    "d = {}\n",
    "for i in range(200): d['var_'+str(i)] = 'float32'\n",
    "d['target'] = 'uint8'\n",
    "d['ID_code'] = 'object'\n",
    "\n",
    "train = pd.read_csv(train_path, dtype=d)\n",
    "test = pd.read_csv(test_path, dtype=d)\n",
    "\n",
    "print('Loaded',len(train),'rows of train')\n",
    "print('Loaded',len(test),'rows of test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "Kwc_vdLc7wx1",
    "outputId": "1542b515-d8fa-41ff-97e7-2c5b2c2efe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-d4pdax0l\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-d4pdax0l\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.4)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101065 sha256=d1f26a4a8c2bb22af674bb2b1ca16a977f7557c4b736ad77cca9277f9b0da30e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9wgbj0q5/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
      "Successfully built keras-contrib\n",
      "--2019-12-01 19:37:16--  https://github.com/jganzabal/santander_kaggle_solutions_tests/raw/master/santander_helper.py\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jganzabal/santander_kaggle_solutions_tests/master/santander_helper.py [following]\n",
      "--2019-12-01 19:37:16--  https://raw.githubusercontent.com/jganzabal/santander_kaggle_solutions_tests/master/santander_helper.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3336 (3.3K) [text/plain]\n",
      "Saving to: ‘santander_helper.py.1’\n",
      "\n",
      "santander_helper.py 100%[===================>]   3.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-12-01 19:37:16 (49.1 MB/s) - ‘santander_helper.py.1’ saved [3336/3336]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "! wget https://github.com/jganzabal/santander_kaggle_solutions_tests/raw/master/santander_helper.py\n",
    "from santander_helper import auc, DataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "import gc\n",
    "from keras.layers import Conv1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "E_eHRJQHH31V",
    "outputId": "f31a60a0-5e77-4429-8913-b54d99ff059c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New 200 features added!\n"
     ]
    }
   ],
   "source": [
    "def encode_FE(df,col,test):\n",
    "    cv = df[col].value_counts()\n",
    "    nm = col+'_FE'\n",
    "    df[nm] = df[col].map(cv)\n",
    "    test[nm] = test[col].map(cv)\n",
    "    test[nm].fillna(0,inplace=True)\n",
    "    if cv.max()<=255:\n",
    "        df[nm] = df[nm].astype('uint8')\n",
    "        test[nm] = test[nm].astype('uint8')\n",
    "    else:\n",
    "        df[nm] = df[nm].astype('uint16')\n",
    "        test[nm] = test[nm].astype('uint16')        \n",
    "    return\n",
    "\n",
    "test['target'] = -1\n",
    "comb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\n",
    "for i in range(200): \n",
    "    encode_FE(comb,'var_'+str(i),test)\n",
    "train = comb[:len(train)]; del comb\n",
    "print('New 200 features added!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rjqktkToIrCV",
    "outputId": "d5c8d2db-87d0-4d54-9c44-d164311097a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_test, real_samples_indexes, synthetic_samples_indexes, unique_count, unique_samples, d\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FXUAMZxJKib"
   },
   "outputs": [],
   "source": [
    "df_train_data = train.drop(columns=['ID_code'])\n",
    "y = df_train_data['target'].values\n",
    "df_train_X = df_train_data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQBPxsu5C5lX"
   },
   "outputs": [],
   "source": [
    "reverse_columns = True\n",
    "if reverse_columns:\n",
    "    reverse_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26,\n",
    "                    27, 29, 32, 35, 37, 40, 41, 47, 48, 49, 51, 52, 53, 55, 60, 61,\n",
    "                    62, 65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94,\n",
    "                    95, 96, 97, 99, 103, 105, 106, 110, 111, 112, 118, 119, 125, 128,\n",
    "                    130, 133, 134, 135, 137, 138, 140, 144, 145, 147, 151, 155, 157,\n",
    "                    159, 161, 162, 163, 164, 167, 168, 170, 171, 173, 175, 176, 179,\n",
    "                    180, 181, 184, 185, 187, 189, 190, 191, 195, 196, 199,\n",
    "                    ]\n",
    "\n",
    "    for j in reverse_list:\n",
    "        df_train_X[f'var_{j}'] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDg5SZ7cJT3T"
   },
   "outputs": [],
   "source": [
    "means = df_train_X.mean(axis=0)\n",
    "stds = df_train_X.std(axis=0)\n",
    "df_train_X_normalized = (df_train_X - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ufWlYdwJWs2"
   },
   "outputs": [],
   "source": [
    "X_train_normalized = np.zeros((df_train_X_normalized.shape[0], 400, 1))\n",
    "for i in range(200):\n",
    "    X_train_normalized[:, 2*i] = df_train_X_normalized[[f'var_{i}']].values #[indexes]\n",
    "    X_train_normalized[:, 2*i+1] = df_train_X_normalized[[f'var_{i}_FE']].values #[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOkfxbghJYyj"
   },
   "outputs": [],
   "source": [
    "def get_model(N_units = 600, kernel_size=2, strides=2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(N_units, kernel_size=kernel_size, strides=strides, padding='valid', \n",
    "                     activation='relu', input_shape=(X_train_normalized.shape[1], 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "hDauSCqOJat2",
    "outputId": "f61e7da8-9775-44ca-919c-2f2ecb25d7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 200, 600)          1800      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 120000)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 120001    \n",
      "=================================================================\n",
      "Total params: 121,801\n",
      "Trainable params: 121,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4kaEQs4fJayP",
    "outputId": "71f6fe92-2ccb-49ff-dd2d-ff3e71a50e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train_data, df_train_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G8GKJFLDJhRa",
    "outputId": "0792b10f-b13f-4c17-dd24-1dd08ee58ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################Fold 0#######################\n",
      "Epoch 1/5\n",
      " - 29s - loss: 0.2750 - auc: 0.8735 - acc: 0.8918 - val_loss: 0.2473 - val_auc: 0.9053 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.90530, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/5\n",
      " - 40s - loss: 0.2398 - auc: 0.9099 - acc: 0.9058 - val_loss: 0.2377 - val_auc: 0.9124 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.90530 to 0.91236, saving model to best_full_model_aux.hdf5\n",
      "Epoch 3/5\n",
      " - 40s - loss: 0.2325 - auc: 0.9162 - acc: 0.9090 - val_loss: 0.2546 - val_auc: 0.9169 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.91236 to 0.91687, saving model to best_full_model_aux.hdf5\n",
      "Epoch 4/5\n",
      " - 41s - loss: 0.2287 - auc: 0.9187 - acc: 0.9108 - val_loss: 0.2347 - val_auc: 0.9180 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.91687 to 0.91797, saving model to best_full_model_aux.hdf5\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.2241 - auc: 0.9220 - acc: 0.9125 - val_loss: 0.2240 - val_auc: 0.9192 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.91797 to 0.91919, saving model to best_full_model_aux.hdf5\n",
      "##################Fold 1#######################\n",
      "Epoch 1/5\n",
      " - 26s - loss: 0.2733 - auc: 0.8761 - acc: 0.8926 - val_loss: 0.2640 - val_auc: 0.8979 - val_acc: 0.8954\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.89793, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/5\n",
      " - 39s - loss: 0.2378 - auc: 0.9114 - acc: 0.9070 - val_loss: 0.2437 - val_auc: 0.9044 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.89793 to 0.90438, saving model to best_full_model_aux.hdf5\n",
      "Epoch 3/5\n",
      " - 40s - loss: 0.2308 - auc: 0.9175 - acc: 0.9098 - val_loss: 0.2348 - val_auc: 0.9099 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.90438 to 0.90986, saving model to best_full_model_aux.hdf5\n",
      "Epoch 4/5\n",
      " - 39s - loss: 0.2272 - auc: 0.9197 - acc: 0.9115 - val_loss: 0.2340 - val_auc: 0.9117 - val_acc: 0.9085\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.90986 to 0.91170, saving model to best_full_model_aux.hdf5\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.2234 - auc: 0.9223 - acc: 0.9130 - val_loss: 0.2397 - val_auc: 0.9131 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.91170 to 0.91314, saving model to best_full_model_aux.hdf5\n",
      "##################Fold 2#######################\n",
      "Epoch 1/5\n",
      " - 27s - loss: 0.2754 - auc: 0.8729 - acc: 0.8918 - val_loss: 0.2385 - val_auc: 0.9073 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.90732, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/5\n",
      " - 39s - loss: 0.2400 - auc: 0.9095 - acc: 0.9057 - val_loss: 0.2335 - val_auc: 0.9120 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.90732 to 0.91200, saving model to best_full_model_aux.hdf5\n",
      "Epoch 3/5\n",
      " - 40s - loss: 0.2338 - auc: 0.9150 - acc: 0.9086 - val_loss: 0.2270 - val_auc: 0.9167 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.91200 to 0.91673, saving model to best_full_model_aux.hdf5\n",
      "Epoch 4/5\n",
      " - 40s - loss: 0.2305 - auc: 0.9174 - acc: 0.9102 - val_loss: 0.2255 - val_auc: 0.9174 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.91673 to 0.91744, saving model to best_full_model_aux.hdf5\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.2253 - auc: 0.9204 - acc: 0.9122 - val_loss: 0.2202 - val_auc: 0.9216 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.91744 to 0.92161, saving model to best_full_model_aux.hdf5\n",
      "##################Fold 3#######################\n",
      "Epoch 1/5\n",
      " - 26s - loss: 0.2730 - auc: 0.8750 - acc: 0.8929 - val_loss: 0.2471 - val_auc: 0.9019 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.90191, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/5\n",
      " - 39s - loss: 0.2384 - auc: 0.9115 - acc: 0.9063 - val_loss: 0.2376 - val_auc: 0.9086 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.90191 to 0.90859, saving model to best_full_model_aux.hdf5\n",
      "Epoch 3/5\n",
      " - 40s - loss: 0.2311 - auc: 0.9171 - acc: 0.9096 - val_loss: 0.2323 - val_auc: 0.9120 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.90859 to 0.91203, saving model to best_full_model_aux.hdf5\n",
      "Epoch 4/5\n",
      " - 40s - loss: 0.2277 - auc: 0.9197 - acc: 0.9109 - val_loss: 0.2333 - val_auc: 0.9138 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.91203 to 0.91378, saving model to best_full_model_aux.hdf5\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.2233 - auc: 0.9222 - acc: 0.9130 - val_loss: 0.2265 - val_auc: 0.9174 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.91378 to 0.91741, saving model to best_full_model_aux.hdf5\n",
      "##################Fold 4#######################\n",
      "Epoch 1/5\n",
      " - 26s - loss: 0.2730 - auc: 0.8760 - acc: 0.8926 - val_loss: 0.2520 - val_auc: 0.9006 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.90057, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/5\n",
      " - 39s - loss: 0.2390 - auc: 0.9109 - acc: 0.9063 - val_loss: 0.2447 - val_auc: 0.9080 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.90057 to 0.90802, saving model to best_full_model_aux.hdf5\n",
      "Epoch 3/5\n",
      " - 40s - loss: 0.2313 - auc: 0.9173 - acc: 0.9094 - val_loss: 0.2329 - val_auc: 0.9115 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.90802 to 0.91148, saving model to best_full_model_aux.hdf5\n",
      "Epoch 4/5\n",
      " - 40s - loss: 0.2276 - auc: 0.9196 - acc: 0.9111 - val_loss: 0.2318 - val_auc: 0.9133 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.91148 to 0.91334, saving model to best_full_model_aux.hdf5\n",
      "Epoch 5/5\n",
      " - 40s - loss: 0.2222 - auc: 0.9224 - acc: 0.9132 - val_loss: 0.2279 - val_auc: 0.9157 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.91334 to 0.91570, saving model to best_full_model_aux.hdf5\n"
     ]
    }
   ],
   "source": [
    "best_model_file_name = 'best_full_model_aux.hdf5'\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "patience = 18\n",
    "epochs = 5\n",
    "bs = 512\n",
    "N_units = 600\n",
    "common_rows = 2\n",
    "class_0_aug = 4\n",
    "class_1_aug = 6\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_X_normalized, y)):\n",
    "    print(f'##################Fold {fold}#######################')\n",
    "    model = get_model(N_units)\n",
    "    model.compile(Adam(), loss='binary_crossentropy', metrics=[auc, 'accuracy'])\n",
    "    es = EarlyStopping(monitor='val_auc', patience=patience, mode='max', verbose=1)\n",
    "    mc = ModelCheckpoint(best_model_file_name, monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n",
    " \n",
    "    generator = DataGenerator(X_train_normalized[trn_idx], y[trn_idx], \n",
    "                              batch_size=bs, shuffle=True, \n",
    "                              class_1_aug=class_1_aug, \n",
    "                              class_0_aug=class_0_aug,\n",
    "                              common_rows = common_rows\n",
    "                             )\n",
    "    tr_iter_in_epoch = generator.__len__()\n",
    "\n",
    "    clr = CyclicLR(base_lr=0.0001, max_lr=0.005, step_size=4*tr_iter_in_epoch, mode='triangular2')\n",
    "    X_val_data, y_val_data = DataGenerator.augment(X_train_normalized[val_idx], \n",
    "                                     y[val_idx], class_1_aug=class_1_aug//2, class_0_aug=class_0_aug//2, common_rows = common_rows)\n",
    "    indexes_val = np.arange(len(y_val_data))\n",
    "    np.random.shuffle(indexes_val)\n",
    "    model.fit_generator(generator,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              callbacks = [es, \n",
    "                           mc, \n",
    "                           clr],\n",
    "              validation_data=(X_val_data[indexes_val], y_val_data[indexes_val])\n",
    "                )\n",
    "    \n",
    "    model = get_model()\n",
    "\n",
    "    model.load_weights(best_model_file_name)\n",
    "    \n",
    "    model.save_weights(f'CNN_generator_fold_{fold}_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYr7Wf6qKbCU"
   },
   "outputs": [],
   "source": [
    "df_test = test.drop(columns=['ID_code', 'target'])\n",
    "if reverse_columns:\n",
    "    for j in reverse_list:\n",
    "        df_test[f'var_{j}'] *= -1\n",
    "df_test_X_normalized = (df_test - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLhwruNdLlwn"
   },
   "outputs": [],
   "source": [
    "X_test_normalized = np.zeros((df_test_X_normalized.shape[0], 400, 1))\n",
    "for i in range(200):\n",
    "    X_test_normalized[:, 2*i] = df_test_X_normalized[[f'var_{i}']].values\n",
    "    X_test_normalized[:, 2*i+1] = df_test_X_normalized[[f'var_{i}_FE']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "Z-GeFTU_KcS3",
    "outputId": "4993912b-e3a5-45a6-9e0b-08435102474a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################Fold 0#######################\n",
      "40001/40001 [==============================] - 2s 54us/step\n",
      "0.9168077167869555\n",
      "200000/200000 [==============================] - 10s 50us/step\n",
      "##################Fold 1#######################\n",
      "40001/40001 [==============================] - 2s 52us/step\n",
      "0.9140942165302556\n",
      "200000/200000 [==============================] - 9s 47us/step\n",
      "##################Fold 2#######################\n",
      "40000/40000 [==============================] - 2s 47us/step\n",
      "0.9215534300426715\n",
      "200000/200000 [==============================] - 10s 48us/step\n",
      "##################Fold 3#######################\n",
      "39999/39999 [==============================] - 2s 47us/step\n",
      "0.915788197418571\n",
      "200000/200000 [==============================] - 10s 48us/step\n",
      "##################Fold 4#######################\n",
      "39999/39999 [==============================] - 2s 48us/step\n",
      "0.9177610940860264\n",
      "200000/200000 [==============================] - 10s 48us/step\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "full_val_preds = np.zeros((len(df_train_X_normalized), 1))\n",
    "model = get_model(N_units)\n",
    "test_predictions = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_X_normalized, y)):\n",
    "    print(f'##################Fold {fold}#######################')\n",
    "    X_train = X_train_normalized[trn_idx]\n",
    "    X_val = X_train_normalized[val_idx]\n",
    "    filename = f'CNN_generator_fold_{fold}_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}.hdf5'\n",
    "    model.load_weights(filename)\n",
    "    full_val_preds[val_idx] = model.predict(X_val, verbose=1)\n",
    "    print(roc_auc_score(y[val_idx], full_val_preds[val_idx]))\n",
    "    test_predictions = test_predictions + model.predict(X_test_normalized, verbose=1)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network gave as AUC of 91.6% which is our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OtcadkxzL3OY",
    "outputId": "a9b3a79d-e06e-4045-8ef8-2cb2c8ecfac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162280696727534"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, full_val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbarCiW5L6CI"
   },
   "outputs": [],
   "source": [
    "def save_submit_file(predictions, filename, test_filename=test_path, index_column='ID_code', target_column = 'target'):\n",
    "    df_test_submit = pd.read_csv(test_filename).set_index(index_column)\n",
    "    df_test_submit[target_column] = predictions\n",
    "    df_test_submit[[target_column]].to_csv(filename)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxsGCjtcD3Hk"
   },
   "outputs": [],
   "source": [
    "save_submit_file(test_predictions, \n",
    "                 f'final.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Deep_Learning_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
