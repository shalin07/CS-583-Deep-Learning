{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 1: Numerical Optimization for Logistic Regression.\n",
    "\n",
    "### Name: [Shalin Barot]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read the lecture note: [click here](https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf)\n",
    "\n",
    "2. Read, complete, and run my code.\n",
    "\n",
    "3. **Implement mini-batch SGD** and evaluate the performance. (1 bonus point.)\n",
    "\n",
    "4. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain **the code** and **the output after execution**.\n",
    "    \n",
    "5. Upload this .HTML file to your Google Drive, Dropbox, or your Github repo.\n",
    "\n",
    "6. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/Bonus1/Bonus1.html\n",
    "\n",
    "\n",
    "## Grading criteria:\n",
    "\n",
    "1. When computing the ```gradient``` and ```objective function value``` using a batch of samples, use **matrix-vector multiplication** rather than a FOR LOOP of **vector-vector multiplications**.\n",
    "\n",
    "2. Plot ```objective function value``` against ```epochs```. In the plot, compare GD, SGD, and MB-SGD (with $b=8$ and $b=64$). The plot must look reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (615, 8)\n",
      "Shape of x_test: (153, 8)\n",
      "Shape of y_train: (615, 1)\n",
      "Shape of y_test: (153, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = int(numpy.ceil(n * 0.8))\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[-0.02220476 -0.02155744 -0.02982632 -0.0137193  -0.05507207 -0.04541228\n",
      "  -0.07443152 -0.13230573]]\n",
      "test std = \n",
      "[[0.9263263  0.84734649 0.96740936 0.9465305  1.03584115 0.9143527\n",
      "  1.08218021 0.9569777 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add a dimension of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (615, 9)\n",
      "Shape of x_test: (153, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     objective function value (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "    loss = numpy.mean(vec2) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    return loss + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial objective function value = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initialize w\n",
    "d = x_train.shape[1]\n",
    "w = numpy.zeros((d, 1))\n",
    "\n",
    "# evaluate the objective function value at w\n",
    "lam = 1E-6\n",
    "objval0 = objective(w, x_train, y_train, lam)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ is $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     g: g: d-by-1 matrix, full gradient\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "    vec3 = -numpy.mean(vec2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    g = vec3 + lam * w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each iteration's objective value\n",
    "def grad_descent(x, y, lam, stepsize, max_iter=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_iter) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= stepsize * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at t=0 is 0.6931471805599453\n",
      "Objective value at t=1 is 0.5853864108938415\n",
      "Objective value at t=2 is 0.5397971277948828\n",
      "Objective value at t=3 is 0.5152605544141639\n",
      "Objective value at t=4 is 0.5000845943967009\n",
      "Objective value at t=5 is 0.48992207900108065\n",
      "Objective value at t=6 is 0.48276955724977044\n",
      "Objective value at t=7 is 0.47756186876506307\n",
      "Objective value at t=8 is 0.4736752042292651\n",
      "Objective value at t=9 is 0.47071873471989323\n",
      "Objective value at t=10 is 0.468435361287539\n",
      "Objective value at t=11 is 0.4666496425131046\n",
      "Objective value at t=12 is 0.4652383823326689\n",
      "Objective value at t=13 is 0.4641130467189034\n",
      "Objective value at t=14 is 0.46320877573696734\n",
      "Objective value at t=15 is 0.46247727038941266\n",
      "Objective value at t=16 is 0.46188205323586157\n",
      "Objective value at t=17 is 0.4613952335804222\n",
      "Objective value at t=18 is 0.46099525358686216\n",
      "Objective value at t=19 is 0.46066528938611245\n",
      "Objective value at t=20 is 0.4603920986582089\n",
      "Objective value at t=21 is 0.4601651781236322\n",
      "Objective value at t=22 is 0.4599761396587991\n",
      "Objective value at t=23 is 0.45981824290515355\n",
      "Objective value at t=24 is 0.45968604139382613\n",
      "Objective value at t=25 is 0.4595751120161399\n",
      "Objective value at t=26 is 0.45948184637465295\n",
      "Objective value at t=27 is 0.4594032885521506\n",
      "Objective value at t=28 is 0.4593370080316881\n",
      "Objective value at t=29 is 0.459280999470179\n",
      "Objective value at t=30 is 0.45923360315402456\n",
      "Objective value at t=31 is 0.45919344150397995\n",
      "Objective value at t=32 is 0.45915936812143093\n",
      "Objective value at t=33 is 0.4591304266985914\n",
      "Objective value at t=34 is 0.4591058177334924\n",
      "Objective value at t=35 is 0.4590848714550028\n",
      "Objective value at t=36 is 0.4590670257146246\n",
      "Objective value at t=37 is 0.45905180786984157\n",
      "Objective value at t=38 is 0.45903881988964695\n",
      "Objective value at t=39 is 0.4590277260719794\n",
      "Objective value at t=40 is 0.4590182428865669\n",
      "Objective value at t=41 is 0.45901013055350526\n",
      "Objective value at t=42 is 0.45900318604407747\n",
      "Objective value at t=43 is 0.458997237250563\n",
      "Objective value at t=44 is 0.45899213811965944\n",
      "Objective value at t=45 is 0.4589877645823657\n",
      "Objective value at t=46 is 0.4589840111438162\n",
      "Objective value at t=47 is 0.4589807880212394\n",
      "Objective value at t=48 is 0.4589780187381502\n",
      "Objective value at t=49 is 0.4589756380990695\n",
      "Objective value at t=50 is 0.4589735904822281\n",
      "Objective value at t=51 is 0.45897182839846434\n",
      "Objective value at t=52 is 0.45897031127333304\n",
      "Objective value at t=53 is 0.45896900441667393\n",
      "Objective value at t=54 is 0.4589678781498447\n",
      "Objective value at t=55 is 0.4589669070657423\n",
      "Objective value at t=56 is 0.4589660694008046\n",
      "Objective value at t=57 is 0.45896534650155835\n",
      "Objective value at t=58 is 0.45896472237109065\n",
      "Objective value at t=59 is 0.45896418328314637\n",
      "Objective value at t=60 is 0.4589637174535141\n",
      "Objective value at t=61 is 0.45896331475998414\n",
      "Objective value at t=62 is 0.4589629665035273\n",
      "Objective value at t=63 is 0.45896266520448836\n",
      "Objective value at t=64 is 0.45896240442854497\n",
      "Objective value at t=65 is 0.45896217863798905\n",
      "Objective value at t=66 is 0.4589619830645706\n",
      "Objective value at t=67 is 0.45896181360071103\n",
      "Objective value at t=68 is 0.4589616667063801\n",
      "Objective value at t=69 is 0.4589615393293365\n",
      "Objective value at t=70 is 0.4589614288367774\n",
      "Objective value at t=71 is 0.4589613329567333\n",
      "Objective value at t=72 is 0.4589612497277952\n",
      "Objective value at t=73 is 0.45896117745596476\n",
      "Objective value at t=74 is 0.45896111467760287\n",
      "Objective value at t=75 is 0.45896106012759785\n",
      "Objective value at t=76 is 0.45896101271200573\n",
      "Objective value at t=77 is 0.4589609714845232\n",
      "Objective value at t=78 is 0.4589609356262475\n",
      "Objective value at t=79 is 0.45896090442825627\n",
      "Objective value at t=80 is 0.4589608772766073\n",
      "Objective value at t=81 is 0.4589608536394175\n",
      "Objective value at t=82 is 0.4589608330557273\n",
      "Objective value at t=83 is 0.4589608151259005\n",
      "Objective value at t=84 is 0.45896079950334234\n",
      "Objective value at t=85 is 0.45896078588735595\n",
      "Objective value at t=86 is 0.458960774016974\n",
      "Objective value at t=87 is 0.45896076366563243\n",
      "Objective value at t=88 is 0.4589607546365696\n",
      "Objective value at t=89 is 0.4589607467588497\n",
      "Objective value at t=90 is 0.4589607398839243\n",
      "Objective value at t=91 is 0.4589607338826597\n",
      "Objective value at t=92 is 0.45896072864276344\n",
      "Objective value at t=93 is 0.45896072406655836\n",
      "Objective value at t=94 is 0.4589607200690538\n",
      "Objective value at t=95 is 0.458960716576276\n",
      "Objective value at t=96 is 0.45896071352382095\n",
      "Objective value at t=97 is 0.45896071085560036\n",
      "Objective value at t=98 is 0.45896070852275306\n",
      "Objective value at t=99 is 0.4589607064827035\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 1.0\n",
    "w, objvals_gd = grad_descent(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi # 1-by-d matrix\n",
    "    yxw = float(numpy.dot(yx, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-yxw)) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + numpy.exp(yxw)) # d-by-1 matrix\n",
    "    g = g_loss + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def sgd(x, y, lam, stepsize, max_epoch=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i, :]) # scalar\n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.5185173265581478\n",
      "Objective value at epoch t=1 is 0.5111181376414775\n",
      "Objective value at epoch t=2 is 0.49943958127685967\n",
      "Objective value at epoch t=3 is 0.4943263080287044\n",
      "Objective value at epoch t=4 is 0.4973971365040499\n",
      "Objective value at epoch t=5 is 0.49614493886841293\n",
      "Objective value at epoch t=6 is 0.4861747795383691\n",
      "Objective value at epoch t=7 is 0.487579409480164\n",
      "Objective value at epoch t=8 is 0.4848032185822282\n",
      "Objective value at epoch t=9 is 0.48103725947588344\n",
      "Objective value at epoch t=10 is 0.47441768729555095\n",
      "Objective value at epoch t=11 is 0.480594661800138\n",
      "Objective value at epoch t=12 is 0.477189964060792\n",
      "Objective value at epoch t=13 is 0.47584447159427873\n",
      "Objective value at epoch t=14 is 0.4726593773272263\n",
      "Objective value at epoch t=15 is 0.46929030269161237\n",
      "Objective value at epoch t=16 is 0.4700887095464399\n",
      "Objective value at epoch t=17 is 0.4695087335916029\n",
      "Objective value at epoch t=18 is 0.46938499407740186\n",
      "Objective value at epoch t=19 is 0.46807573307247624\n",
      "Objective value at epoch t=20 is 0.46580231920079335\n",
      "Objective value at epoch t=21 is 0.46708669320099394\n",
      "Objective value at epoch t=22 is 0.4654082380341778\n",
      "Objective value at epoch t=23 is 0.46471670739939896\n",
      "Objective value at epoch t=24 is 0.46480423751600286\n",
      "Objective value at epoch t=25 is 0.46408806594141533\n",
      "Objective value at epoch t=26 is 0.4632122789280054\n",
      "Objective value at epoch t=27 is 0.4630516915542931\n",
      "Objective value at epoch t=28 is 0.46285059042965093\n",
      "Objective value at epoch t=29 is 0.46234470457785953\n",
      "Objective value at epoch t=30 is 0.4620852618298214\n",
      "Objective value at epoch t=31 is 0.46171895725672923\n",
      "Objective value at epoch t=32 is 0.4614369426848044\n",
      "Objective value at epoch t=33 is 0.4612216377400438\n",
      "Objective value at epoch t=34 is 0.46098863669716134\n",
      "Objective value at epoch t=35 is 0.4608212493462012\n",
      "Objective value at epoch t=36 is 0.46062435385610495\n",
      "Objective value at epoch t=37 is 0.460447193805939\n",
      "Objective value at epoch t=38 is 0.4603117312069228\n",
      "Objective value at epoch t=39 is 0.4601901002902336\n",
      "Objective value at epoch t=40 is 0.4600635870829161\n",
      "Objective value at epoch t=41 is 0.459941401910819\n",
      "Objective value at epoch t=42 is 0.4598637846664099\n",
      "Objective value at epoch t=43 is 0.45977487188301247\n",
      "Objective value at epoch t=44 is 0.45968756152803086\n",
      "Objective value at epoch t=45 is 0.45961764404448313\n",
      "Objective value at epoch t=46 is 0.45955020058431156\n",
      "Objective value at epoch t=47 is 0.4594921805477887\n",
      "Objective value at epoch t=48 is 0.4594421659600958\n",
      "Objective value at epoch t=49 is 0.45939189657525514\n",
      "Objective value at epoch t=50 is 0.45934807832854313\n",
      "Objective value at epoch t=51 is 0.45931300711880274\n",
      "Objective value at epoch t=52 is 0.45927676756940866\n",
      "Objective value at epoch t=53 is 0.4592440755326585\n",
      "Objective value at epoch t=54 is 0.4592156432405317\n",
      "Objective value at epoch t=55 is 0.459190418986756\n",
      "Objective value at epoch t=56 is 0.4591686562213888\n",
      "Objective value at epoch t=57 is 0.45914806067793446\n",
      "Objective value at epoch t=58 is 0.45912955196851757\n",
      "Objective value at epoch t=59 is 0.45911261310905555\n",
      "Objective value at epoch t=60 is 0.45909710437420564\n",
      "Objective value at epoch t=61 is 0.4590838048673587\n",
      "Objective value at epoch t=62 is 0.4590712892939759\n",
      "Objective value at epoch t=63 is 0.4590604829895128\n",
      "Objective value at epoch t=64 is 0.45905067005409644\n",
      "Objective value at epoch t=65 is 0.4590417079003915\n",
      "Objective value at epoch t=66 is 0.4590336761276314\n",
      "Objective value at epoch t=67 is 0.4590263319127226\n",
      "Objective value at epoch t=68 is 0.4590198872495931\n",
      "Objective value at epoch t=69 is 0.4590140310462223\n",
      "Objective value at epoch t=70 is 0.4590086574040276\n",
      "Objective value at epoch t=71 is 0.45900396130926907\n",
      "Objective value at epoch t=72 is 0.45899967341439896\n",
      "Objective value at epoch t=73 is 0.4589958265125412\n",
      "Objective value at epoch t=74 is 0.4589923442402548\n",
      "Objective value at epoch t=75 is 0.4589892392476218\n",
      "Objective value at epoch t=76 is 0.45898642822889535\n",
      "Objective value at epoch t=77 is 0.4589838946388652\n",
      "Objective value at epoch t=78 is 0.4589816166154514\n",
      "Objective value at epoch t=79 is 0.45897957141737394\n",
      "Objective value at epoch t=80 is 0.45897772562940176\n",
      "Objective value at epoch t=81 is 0.4589760592481114\n",
      "Objective value at epoch t=82 is 0.45897457194220487\n",
      "Objective value at epoch t=83 is 0.4589732272324592\n",
      "Objective value at epoch t=84 is 0.4589720180364391\n",
      "Objective value at epoch t=85 is 0.45897092839618314\n",
      "Objective value at epoch t=86 is 0.45896994727457713\n",
      "Objective value at epoch t=87 is 0.458969063269285\n",
      "Objective value at epoch t=88 is 0.4589682706047643\n",
      "Objective value at epoch t=89 is 0.45896755583571325\n",
      "Objective value at epoch t=90 is 0.4589669123015167\n",
      "Objective value at epoch t=91 is 0.4589663333863622\n",
      "Objective value at epoch t=92 is 0.4589658121812243\n",
      "Objective value at epoch t=93 is 0.45896534336563777\n",
      "Objective value at epoch t=94 is 0.4589649212374327\n",
      "Objective value at epoch t=95 is 0.4589645409821056\n",
      "Objective value at epoch t=96 is 0.4589641993828997\n",
      "Objective value at epoch t=97 is 0.45896389159658696\n",
      "Objective value at epoch t=98 is 0.4589636146090726\n",
      "Objective value at epoch t=99 is 0.45896336538642885\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 0.1\n",
    "w, objvals_sgd = sgd(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD with SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd))\n",
    "epochs_sgd = range(len(objvals_sgd))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd, '--b', LineWidth=4)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd, '-r', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1], ['GD', 'SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.21951219512195122\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error_train = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.2549019607843137\n"
     ]
    }
   ],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error_test = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mini-batch SGD (fill the code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Compute the objective $Q_I$ and its gradient using a batch of samples\n",
    "\n",
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: b-by-d matrix\n",
    "#     yi: b-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def mb_stochastic_objective_gradient(w, xi, yi, lam, b):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of stochastic_objective_gradient\n",
    "    # Use matrix-vector multiplication; do not use FOR LOOP of vector-vector multiplications\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi\n",
    "    yxw = float(numpy.dot(yx * w))\n",
    "    loss = numpy.mean(numpy.log(1 + numpy.exp(-yxw))).reshape(d,1)\n",
    "    reg = lam / 2 * numpy.sum(w*w) \n",
    "    obj = loss + reg\n",
    "    \n",
    "    g_loss = numpy.mean(-yx.T/1 + numpy.exp(yxw)).reshape(d,1)\n",
    "    g = g_loss + lam * w\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Implement mini-batch SGD\n",
    "\n",
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_stochastic_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on.\n",
    "3. Note that ``n mod b`` may not be zero. For example, if $n=1095$ and $b=100$, then there will be $95$ samples left. If the $95$ samples are not used, then the computed objective function will be incorrect. You need to take this effect into account in your implementation of the ``mb_sgd`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "\n",
    "import random\n",
    "\n",
    "def mb_sgd(x, y, lam, b, stepsize, max_epoch=100, w=None):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of sgd\n",
    "    # Record one objective value per epoch (not per iteration!)\n",
    "    n , d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch)\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d,1))\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "#         print(rand_indices)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        objval = 0\n",
    "        for i in range(int(n/b)):\n",
    "            xi = x_rand[:b, :]\n",
    "            yi = float(y_rand[:b, :])\n",
    "            \n",
    "            obj , g = mb_stochastic_objective_gradient(w , xi, yi, lam, b)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "            a=b\n",
    "            b += b\n",
    "            \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Run MB-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB-SGD with batch size b=8\n",
    "lam = 1E-6 # do not change\n",
    "b = 8 # do not change\n",
    "stepsize = 0.1 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd8 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB-SGD with batch size b=64\n",
    "lam = 1E-6 # do not change\n",
    "b = 64 # do not change\n",
    "stepsize = 0.1 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd64 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot and compare GD, SGD, and MB-SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to compare the following algorithms:\n",
    "\n",
    "- Gradient descent (GD)\n",
    "\n",
    "- SGD\n",
    "\n",
    "- MB-SGD with b=8\n",
    "\n",
    "- MB-SGD with b=64\n",
    "\n",
    "Follow the code in Section 4 to plot ```objective function value``` against ```epochs```. There should be four curves in the plot; each curve corresponds to one algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Logistic regression with $\\ell_2$-norm regularization is a strongly convex optimization problem. All the algorithms will converge to the same solution. In the end, the ``objective function value`` of the 4 algorithms will be the same. If not the same, your implementation must be wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 4 curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
